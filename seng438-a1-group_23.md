>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 23     |
|---------------|
| Hamza Niaz    |   
| Bilal Pasha   |   
| Yousef Hammad |   
| Issam Akthar  |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

An ATM (automated teller machine) is defined as a computer that allows for customers of a bank company to complete bank transactions without the need for a branch representative. In this lab we are going to test the integrity of an ATM simulator by using testing strategies such as the exploratory and manual scripted testing. The exploratory phase will allow us to freely test the ATM system in any way our team decides in order to find bugs. In order for our team to properly execute any exploratory test we will first write down a plan in which we decide how the group will attempt to test the software. Meanwhile, for the manual scripted phase of this lab report, we will be testing a set of predefined test cases and recording them when one expected output for a test does not match the actual value of the simulation (a bug). Once a bug is found, our team will report it on jira and export this list on an excel sheet with all the details needed to replicate each bug. Once our team is finished with all the test cases, we will then explain the benefits, tradeoffs, effectiveness, and efficiency between the exploratory and manual tests. Additionally, In this excel sheet we will include the regression testing results by attempting to replicate the bug in the new updated software program ATM 1.1 to review which errors are fixed and which ones are still an issue for the system. Furthermore, once we have reviewed all our bugs with the new updated software, we will then go into further detail about our results from the new software.

# High-level description of the exploratory testing plan
The way we are deciding to test this strategy is to explore each aspect of the application based on the familiarization steps given to us in the README.md. Well start off with testing the twenty dollar bill deposit write and read function when turning the application on. Next will be the credit card read/write prompt given by the system when the  “Click to insert card button” is pressed. Once we feel like we have explored everything with these prompts, we'll move on to testing the pin verification function to figure out how secure the ATM program is.  From there we will test each transaction functionality withdraw, deposit, transfer, and balance inquiry. For these tests we will record all of our tests for card number 1 and replicate them for card number 2 to see how different accounts respond to the ATM machine. For the withdrawal functionality we have to take into account how much money we have deposited into the bank and the amount on the client's card as well. With this information we will test the withdrawal function with various inputs such as requesting more money than the card holder has, inputting values that satisfies the card but not the bank, etc.. The deposit functionality will be a simple process to test by inputting various amounts of large and small inputs and checking if the receipt/log corresponds with the correct results. The transfer function would need to rely on the amount of money in each card's accounts making testing cards 1 and 2 essential due to card 2 having access to the money market account and card 1 having access to savings. Balance inquiry will require us to test the other transaction functions to see how the ATM machine responds. Everytime our team decides to perform a test we will wait till we receive an output, record and then close the application entirely in order to ensure that each test we perform has no outlying factor.

# Comparison of exploratory and manual functional testing
In our findings, we discovered six tests that did not pass the manual testing phase. The first test that failed was a withdrawal issue, where the system asks the customer to choose a dollar amount to withdraw. Upon entering $20 to withdraw from the account, the expected output would be $80 remaining in the account. However, the ATM system deducted $40 and $60 was remaining in the account. The system deducts double the amount that the user asks to withdraw. The second test that failed was a deposit issue. When the user inputs the amount to deposit, the system only actually deposits half the amount entered. In our testing, $20 was deposited into the checking account, however the actual outcome determined that only $10 was added to the account balance. The third test that failed was a transfer issue, where the system is performing a transfer transaction from 2 account types. In this case, a transfer of $20 was made from the checking to the savings account. The expected outcome would see the savings account have a total balance of $1020. However, there are 2 errors in the receipt. Firstly, the transfer message is in the incorrect order in that it says the transfer was from the savings to the checking account when it was originally checking to the savings account. The second problem with the transfer is that it transferred $19.50 rather than the total $20, making the total balance $1019.50 rather than $1020. The final three tests that failed were all in relation to the invalid PIN extension. When the user incorrectly enters the PIN, they are then prompted to re-enter the correct password. If they re-enter the password correctly, they are prompted to enter the password, which is the same as the original enter password menu. This is an error as the user should not be prompted to enter the password if they re-entered the password correctly.

After testing both the manual and exploratory test strategies, our team encountered many benefits and tradeoffs. One of the main benefits from exploratory testing was that each team was able to spend a good amount of time developing an understanding of how the software operates. This is done by testing each functionality to its limits by attempting to input various amounts of values to retrieve different data and errors within the software. Finding these errors on our own made us understand what specific aspects of the ATM simulation had errors. An example would be finding the typo error “wood” that is consistent within all transaction options making it a more global issue compared to the faulty withdrawals and deposit transactions bugs we were facing. Exploratory testing also allows us to prioritize any function we need without following a script. This results in more investigations about aspects of the code we would not have been able to find, such as the frozen atm screen when inputting a significantly large value for the deposit, which would not have been found if we decided to move on and not attempt various inputs in the deposit transaction. This makes the effectiveness of exploratory testing to be extremely high. However, the most significant tradeoff was the efficiency of this testing strategy. Having to develop tests on our own and keeping track of how to repeat the steps to activate the bug became a tedious and later complicated task, especially when the previous inputs for functions would affect other future functions. Furthermore, due to our team being a pair of testers, our results would vary extremely due to our plan allowing us to prioritize whatever we would want. Additionally, prioritizing tasks allows for other functions to become unrecognized, resulting in these unprioritized functions to have little to no tests for bugs.

Manual scripted testing allows for more organized tests to be accomplished making efficiency one of its greatest strengths. By having a list of tests that need to be done, our group split the work evenly. The list of tests provided made communication extremely clear between test partners as they both followed what was given to them in concise instructions for the instructions being tested, the initial state of the system, the input required, and the expected output. In addition to the tests, each function within the system received an equal amount of tests resulting in the system having a consistent pace while testing. Manual testing is also based on specifications resulting in these tests becoming extremely beneficial to the developer of the code, especially when being provided to numerous testers. Furthermore, when multiple testers are following and providing information on outputs of the script, this allows for measurable information which can be used to determine which priority does each function have compared to one another. This can be seen with the amount of new bugs we have found in the “validate PIN” function, this is because we did not prioritize this function and instead focused on other aspects of the software. The main problem when it comes to scripted testability is the testers lack of understanding of the system. This is due to the testers having no freedom to decide what functions they want to test in order to develop a better grasp on how the ATM application operates. So by making the testers focus on one test and proceed with the next one, it provides the tester with the mindset to finish each test without much care which can lead to faulty results.

# Notes and discussion of the peer reviews of defect reports
Upon reviewing the defect report from our team’s other pair, we discovered many similarities in our test cases. Both of the pairs were mainly focused on the main function of the ATM system application. Those functions being withdrawal, deposit, transfer, and account inquiry. This is most likely due to these operations being the most important in a real life ATM system and they must be accurate like the real-world systems. Both of the reports were easy to replicate and follow, which helped us locate the errors quickly.Although there were many similarities, there were also a few differences such as the amount of money tested for withdrawals and deposits was different in our test cases. This helped us identify some of the mathematical errors of this system. Having two different defect reports allowed us to notice details that the other team missed and provided a more comprehensive set of test cases.

# How the pair testing was managed and team work/effort was divided 
Our team used Discord to communicate for Assignment 1. We created our own Discord server to talk about the assignments and share our ideas. We also use Google Drive to share documents. One member of our group shared the Github repository and a Jira project with everyone in the group to work on it together. We decided to work in the following pairs:
Pair 1: Bilal Pasha (Analyzer) & Issam Akthar (Tester)
Pair 2: Hamza Niaz (Analyzer)  & Yousef Hammad (Tester)

In exploratory testing, each pair decided to conduct and find their own test using the agreed plan written above. After thirty minutes of testing we came together as a team to review all of the tests we have done and what bugs were found from them. We then agreed upon what were considered bugs and reported them on the bug tracking tool. This led to us finding over fifteen total errors within the system.

For the manual scripted testing, we divided  the work evenly and decided to work as a tester 
(partner 1) and an analyzer(partner 2). The tester would share their screen on Discord with the appropriate version of the system for the appropriate type of test and the developer was creating the issues in the Backlog system with the correct information being reported. The analyzer would also share their screen to ensure that the correct information is being recorded with the correct label (MFT or Exploratory).

# Difficulties encountered, challenges overcome, and lessons learned
In the beginning of the assignment, it was hard to start as the instructions were long and not very clear. It also took some time to understand the bug tracking software as it is our first time using such a software. The exploratory test plan took much longer than expected. Writing proper and concise information about each bug was difficult. Testing the ATM system application took a lot longer than it should have due to the GUI animations. Some lessons that we learned were how to properly use Backlog as a bug tracking tool and proper ways of how to report a bug.

# Comments/feedback on the lab and lab document itself
The lab session was chaotic and we were unsure of what to do initially. The lab documents were confusing at first, but as we progressed and asked questions, we began to understand the tasks at hand. However, the instructions in the lab document were often unclear, which made it difficult to know how to proceed. Our group members worked together to interpret the lab assignment as best as we could. Using GitHub Classroom was a positive aspect of the lab as it allowed us to become familiar with Git, which is becoming a widely used tool in the industry. Familiarizing ourselves with Git as students will be beneficial for future work. Additionally, using Git made it easier for our group to keep track of any changes to files. As we move on to later labs where we will write our own test cases in JUnit, it will become more efficient.
